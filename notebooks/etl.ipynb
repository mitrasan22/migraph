{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2b31c5",
   "metadata": {},
   "source": [
    "# ETL Pipeline (Processed Graph)\n",
    "\n",
    "Builds a richer, connected graph from COVID + Finance + World Bank + Wikipedia samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd18cf0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:37.623587Z",
     "iopub.status.busy": "2026-02-07T07:44:37.623587Z",
     "iopub.status.idle": "2026-02-07T07:44:38.206583Z",
     "shell.execute_reply": "2026-02-07T07:44:38.205738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\n",
      "Processed: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import re\n",
    "import bz2\n",
    "import io\n",
    "import urllib.request\n",
    "\n",
    "root = Path.cwd()\n",
    "if root.name == 'notebooks':\n",
    "    root = root.parent\n",
    "\n",
    "raw = root / 'data' / 'raw'\n",
    "processed = root / 'data' / 'processed'\n",
    "processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(raw / 'world_bank').mkdir(parents=True, exist_ok=True)\n",
    "(raw / 'finance').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Raw:', raw)\n",
    "print('Processed:', processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bae53",
   "metadata": {},
   "source": [
    "## Download World Bank indicators (multi?country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a03bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.206583Z",
     "iopub.status.busy": "2026-02-07T07:44:38.206583Z",
     "iopub.status.idle": "2026-02-07T07:44:38.223326Z",
     "shell.execute_reply": "2026-02-07T07:44:38.222044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\world_bank\\world_bank_gdp_multi.zip\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\world_bank\\world_bank_gdp_pc_multi.zip\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\world_bank\\world_bank_pop_multi.zip\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\world_bank\\world_bank_inflation_multi.zip\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\world_bank\\world_bank_unemployment_multi.zip\n"
     ]
    }
   ],
   "source": [
    "wb_specs = [\n",
    "    ('world_bank_gdp_multi.zip', 'NY.GDP.MKTP.CD'),\n",
    "    ('world_bank_gdp_pc_multi.zip', 'NY.GDP.PCAP.CD'),\n",
    "    ('world_bank_pop_multi.zip', 'SP.POP.TOTL'),\n",
    "    ('world_bank_inflation_multi.zip', 'FP.CPI.TOTL.ZG'),\n",
    "    ('world_bank_unemployment_multi.zip', 'SL.UEM.TOTL.ZS'),\n",
    "]\n",
    "countries = 'USA;IND;BRA;CHN;DEU;JPN;GBR;FRA;CAN;AUS'\n",
    "for filename, indicator in wb_specs:\n",
    "    url = f'https://api.worldbank.org/v2/country/{countries}/indicator/{indicator}?downloadformat=csv'\n",
    "    out = raw / 'world_bank' / filename\n",
    "    if not out.exists():\n",
    "        print('Downloading:', url)\n",
    "        urllib.request.urlretrieve(url, out)\n",
    "    else:\n",
    "        print('Already exists:', out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ef6d9",
   "metadata": {},
   "source": [
    "## Download Finance tickers (Stooq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0628f9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.223326Z",
     "iopub.status.busy": "2026-02-07T07:44:38.223326Z",
     "iopub.status.idle": "2026-02-07T07:44:38.238825Z",
     "shell.execute_reply": "2026-02-07T07:44:38.238825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\finance\\aapl.us.csv\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\finance\\msft.us.csv\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\finance\\amzn.us.csv\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\finance\\goog.us.csv\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\finance\\tsla.us.csv\n",
      "Already exists: C:\\Users\\SANTANU\\Downloads\\migraph\\data\\raw\\finance\\meta.us.csv\n"
     ]
    }
   ],
   "source": [
    "tickers = ['aapl.us', 'msft.us', 'amzn.us', 'goog.us', 'tsla.us', 'meta.us']\n",
    "for t in tickers:\n",
    "    url = f'https://stooq.com/q/d/l/?s={t}&i=d'\n",
    "    out = raw / 'finance' / f'{t}.csv'\n",
    "    if not out.exists():\n",
    "        print('Downloading:', url)\n",
    "        urllib.request.urlretrieve(url, out)\n",
    "    else:\n",
    "        print('Already exists:', out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aba61e",
   "metadata": {},
   "source": [
    "## Load COVID (OWID) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af09bb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.238825Z",
     "iopub.status.busy": "2026-02-07T07:44:38.238825Z",
     "iopub.status.idle": "2026-02-07T07:44:38.273511Z",
     "shell.execute_reply": "2026-02-07T07:44:38.272302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID nodes: 246\n"
     ]
    }
   ],
   "source": [
    "covid_path = raw / 'covid' / 'owid-covid-latest.csv'\n",
    "covid = pd.read_csv(covid_path)\n",
    "covid = covid[['location','total_cases','total_deaths','population']].dropna().head(250)\n",
    "covid['id'] = covid['location'].str.lower().str.replace(' ', '_')\n",
    "covid['type'] = 'country'\n",
    "covid['cases_per_m'] = covid['total_cases'] / (covid['population'] / 1_000_000)\n",
    "covid['deaths_per_m'] = covid['total_deaths'] / (covid['population'] / 1_000_000)\n",
    "covid['attributes'] = covid.apply(lambda r: {\n",
    "    'total_cases': float(r.total_cases),\n",
    "    'total_deaths': float(r.total_deaths),\n",
    "    'population': float(r.population),\n",
    "    'cases_per_m': float(r.cases_per_m),\n",
    "    'deaths_per_m': float(r.deaths_per_m),\n",
    "}, axis=1)\n",
    "covid_nodes = covid[['id','location','type','attributes']].rename(columns={'location':'label'})\n",
    "print('COVID nodes:', len(covid_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5271aa5",
   "metadata": {},
   "source": [
    "## Load Finance (multi?ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd76a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.273511Z",
     "iopub.status.busy": "2026-02-07T07:44:38.273511Z",
     "iopub.status.idle": "2026-02-07T07:44:38.289673Z",
     "shell.execute_reply": "2026-02-07T07:44:38.288751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance tickers: 6\n"
     ]
    }
   ],
   "source": [
    "tickers = ['aapl.us', 'msft.us', 'amzn.us', 'goog.us', 'tsla.us', 'meta.us']\n",
    "fin_nodes = []\n",
    "for t in tickers:\n",
    "    sym = t.split('.')[0].upper()\n",
    "    fin_nodes.append({'id': sym.lower(), 'label': sym, 'type': 'ticker', 'attributes': {}})\n",
    "fin_nodes = pd.DataFrame(fin_nodes)\n",
    "print('Finance tickers:', len(fin_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80b28f",
   "metadata": {},
   "source": [
    "## Load World Bank indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b230c414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.289673Z",
     "iopub.status.busy": "2026-02-07T07:44:38.289673Z",
     "iopub.status.idle": "2026-02-07T07:44:38.407742Z",
     "shell.execute_reply": "2026-02-07T07:44:38.407742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WB indicator nodes: 562\n"
     ]
    }
   ],
   "source": [
    "def load_wb_zip(path):\n",
    "    with zipfile.ZipFile(path, 'r') as zf:\n",
    "        data_csv = [n for n in zf.namelist() if n.startswith('API_') and n.endswith('.csv')][0]\n",
    "        with zf.open(data_csv) as f:\n",
    "            text = io.TextIOWrapper(f, encoding='utf-8-sig')\n",
    "            return pd.read_csv(text, skiprows=4)\n",
    "\n",
    "wb_gdp = load_wb_zip(raw / 'world_bank' / 'world_bank_gdp_multi.zip')\n",
    "wb_gdp_pc = load_wb_zip(raw / 'world_bank' / 'world_bank_gdp_pc_multi.zip')\n",
    "wb_pop = load_wb_zip(raw / 'world_bank' / 'world_bank_pop_multi.zip')\n",
    "wb_infl = load_wb_zip(raw / 'world_bank' / 'world_bank_inflation_multi.zip')\n",
    "wb_unemp = load_wb_zip(raw / 'world_bank' / 'world_bank_unemployment_multi.zip')\n",
    "\n",
    "def build_indicator_nodes(df, indicator_code, prefix):\n",
    "    nodes = []\n",
    "    for _, row in df[df['Indicator Code']==indicator_code].iterrows():\n",
    "        cc = row['Country Code']\n",
    "        cname = row['Country Name']\n",
    "        years = [c for c in df.columns if re.match(r'^\\d{4}$', str(c))]\n",
    "        years = sorted(years)[-10:]\n",
    "        records = []\n",
    "        for y in years:\n",
    "            val = row[y]\n",
    "            if pd.notna(val):\n",
    "                records.append({'year': y, 'value': float(val)})\n",
    "        nodes.append({\n",
    "            'id': f'{prefix}_{cc.lower()}',\n",
    "            'label': f'{cname} {prefix.upper()}',\n",
    "            'type': 'wb_indicator',\n",
    "            'attributes': {'years': records, 'code': indicator_code},\n",
    "            'country_code': cc,\n",
    "            'country_name': cname,\n",
    "        })\n",
    "    return pd.DataFrame(nodes)\n",
    "\n",
    "gdp_nodes = build_indicator_nodes(wb_gdp, 'NY.GDP.MKTP.CD', 'gdp')\n",
    "gdp_pc_nodes = build_indicator_nodes(wb_gdp_pc, 'NY.GDP.PCAP.CD', 'gdp_pc')\n",
    "pop_nodes = build_indicator_nodes(wb_pop, 'SP.POP.TOTL', 'pop')\n",
    "infl_nodes = build_indicator_nodes(wb_infl, 'FP.CPI.TOTL.ZG', 'inflation')\n",
    "unemp_nodes = build_indicator_nodes(wb_unemp, 'SL.UEM.TOTL.ZS', 'unemployment')\n",
    "wb_nodes = pd.concat([gdp_nodes, gdp_pc_nodes, pop_nodes, infl_nodes, unemp_nodes], ignore_index=True)\n",
    "print('WB indicator nodes:', len(wb_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501a8ba",
   "metadata": {},
   "source": [
    "## Load Wikipedia sample (Simple English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc8a809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.407742Z",
     "iopub.status.busy": "2026-02-07T07:44:38.407742Z",
     "iopub.status.idle": "2026-02-07T07:44:38.648143Z",
     "shell.execute_reply": "2026-02-07T07:44:38.648143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki topics: 500\n"
     ]
    }
   ],
   "source": [
    "wiki_path = raw / 'wikipedia' / 'simplewiki-latest-pages-articles.xml.bz2'\n",
    "topics = []\n",
    "with bz2.open(wiki_path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        if '<title>' in line:\n",
    "            title = line.strip().replace('<title>','').replace('</title>','')\n",
    "            if title and ':' not in title:\n",
    "                topics.append(title)\n",
    "        if len(topics) >= 500:\n",
    "            break\n",
    "\n",
    "wiki_nodes = pd.DataFrame([\n",
    "    {'id': f'wiki_{i}', 'label': t, 'type': 'wiki_topic', 'attributes': {}}\n",
    "    for i, t in enumerate(topics)\n",
    "])\n",
    "print('Wiki topics:', len(wiki_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39ef56",
   "metadata": {},
   "source": [
    "## Keyword extraction from wiki topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf8fea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.648143Z",
     "iopub.status.busy": "2026-02-07T07:44:38.648143Z",
     "iopub.status.idle": "2026-02-07T07:44:38.663989Z",
     "shell.execute_reply": "2026-02-07T07:44:38.663989Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = set(['the','of','and','in','to','a','an','for','on','by','with','at','from','as','is','are','was','were','it'])\n",
    "def keywords(title):\n",
    "    toks = re.findall(r'[A-Za-z]{3,}', title.lower())\n",
    "    toks = [t for t in toks if t not in stop]\n",
    "    return toks[:5]\n",
    "\n",
    "wiki_nodes['keywords'] = wiki_nodes['label'].apply(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbce129",
   "metadata": {},
   "source": [
    "## Derived Nodes (COVID burden + population buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b75010c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.663989Z",
     "iopub.status.busy": "2026-02-07T07:44:38.663989Z",
     "iopub.status.idle": "2026-02-07T07:44:38.689306Z",
     "shell.execute_reply": "2026-02-07T07:44:38.688382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burden nodes: 246\n",
      "Bucket nodes: 4\n"
     ]
    }
   ],
   "source": [
    "burden_nodes = []\n",
    "bucket_nodes = []\n",
    "for _, row in covid.iterrows():\n",
    "    bid = f'covid_burden_{row.id}'\n",
    "    burden_nodes.append({\n",
    "        'id': bid,\n",
    "        'label': f'COVID burden {row.location}',\n",
    "        'type': 'covid_burden',\n",
    "        'attributes': {\n",
    "            'cases_per_m': float(row.cases_per_m),\n",
    "            'deaths_per_m': float(row.deaths_per_m),\n",
    "        },\n",
    "    })\n",
    "\n",
    "    pop = float(row.population)\n",
    "    if pop < 5_000_000:\n",
    "        bucket = 'pop_lt_5m'\n",
    "    elif pop < 20_000_000:\n",
    "        bucket = 'pop_5m_20m'\n",
    "    elif pop < 100_000_000:\n",
    "        bucket = 'pop_20m_100m'\n",
    "    else:\n",
    "        bucket = 'pop_gt_100m'\n",
    "\n",
    "    bucket_nodes.append({\n",
    "        'id': bucket,\n",
    "        'label': bucket.replace('_',' '),\n",
    "        'type': 'population_bucket',\n",
    "        'attributes': {},\n",
    "    })\n",
    "\n",
    "burden_nodes = pd.DataFrame(burden_nodes)\n",
    "bucket_nodes = pd.DataFrame(bucket_nodes).drop_duplicates(subset=['id'])\n",
    "print('Burden nodes:', len(burden_nodes))\n",
    "print('Bucket nodes:', len(bucket_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163224f6",
   "metadata": {},
   "source": [
    "## Build Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dbc2d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:38.689306Z",
     "iopub.status.busy": "2026-02-07T07:44:38.689306Z",
     "iopub.status.idle": "2026-02-07T07:44:42.006634Z",
     "shell.execute_reply": "2026-02-07T07:44:42.006634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote nodes.parquet: 9807\n"
     ]
    }
   ],
   "source": [
    "def _year_columns(df):\n",
    "    years = [int(c) for c in df.columns if re.match(r'^\\d{4}$', str(c))]\n",
    "    return sorted(years)\n",
    "\n",
    "def build_year_nodes(df, indicator_code, prefix, max_years=6):\n",
    "    years = _year_columns(df)[-max_years:]\n",
    "    nodes = []\n",
    "    for _, row in df[df['Indicator Code']==indicator_code].iterrows():\n",
    "        cname = row['Country Name']\n",
    "        cc = row['Country Code']\n",
    "        cid = cname.lower().replace(' ', '_')\n",
    "        indicator_id = f'{prefix}_{cc.lower()}'\n",
    "        for y in years:\n",
    "            y_str = str(y)\n",
    "            val = row.get(y_str)\n",
    "            if pd.notna(val):\n",
    "                nodes.append({\n",
    "                    'id': f'{prefix}_year:{cid}:{y_str}',\n",
    "                    'label': f'{prefix.upper()} {y_str} ({cname})',\n",
    "                    'type': f'{prefix}_year',\n",
    "                    'attributes': {\n",
    "                        'year': int(y),\n",
    "                        'value': float(val),\n",
    "                        'indicator': indicator_code,\n",
    "                        'country_name': cname,\n",
    "                        'country_code': cc,\n",
    "                        'country_id': cid,\n",
    "                        'indicator_id': indicator_id,\n",
    "                    },\n",
    "                })\n",
    "    return pd.DataFrame(nodes)\n",
    "\n",
    "def build_recovery_nodes(df, indicator_code, prefix, base_year=2019, higher_is_better=True):\n",
    "    years = _year_columns(df)\n",
    "    nodes = []\n",
    "    base_col = str(base_year)\n",
    "    if base_col not in df.columns:\n",
    "        return pd.DataFrame(nodes)\n",
    "    for _, row in df[df['Indicator Code']==indicator_code].iterrows():\n",
    "        cname = row['Country Name']\n",
    "        cc = row['Country Code']\n",
    "        cid = cname.lower().replace(' ', '_')\n",
    "        indicator_id = f'{prefix}_{cc.lower()}'\n",
    "        base_val = row.get(base_col)\n",
    "        if pd.isna(base_val):\n",
    "            continue\n",
    "        recovery_year = None\n",
    "        for y in years:\n",
    "            if y <= base_year:\n",
    "                continue\n",
    "            val = row.get(str(y))\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if higher_is_better:\n",
    "                ok = val >= base_val\n",
    "            else:\n",
    "                ok = val <= base_val\n",
    "            if ok:\n",
    "                recovery_year = y\n",
    "                break\n",
    "        if recovery_year is None:\n",
    "            continue\n",
    "        nodes.append({\n",
    "            'id': f'{prefix}_recovery:{cid}',\n",
    "            'label': f'{prefix.upper()} recovery {recovery_year} ({cname})',\n",
    "            'type': f'{prefix}_recovery',\n",
    "            'attributes': {\n",
    "                'year': int(recovery_year),\n",
    "                'base_year': int(base_year),\n",
    "                'base_value': float(base_val),\n",
    "                'indicator': indicator_code,\n",
    "                'country_name': cname,\n",
    "                'country_code': cc,\n",
    "                'country_id': cid,\n",
    "                'indicator_id': indicator_id,\n",
    "            },\n",
    "        })\n",
    "    return pd.DataFrame(nodes)\n",
    "\n",
    "# COVID time-series nodes (yearly rollups)\n",
    "covid_ts_path = raw / 'covid' / 'owid-covid-data.csv'\n",
    "covid_year_nodes = pd.DataFrame([])\n",
    "covid_peak_nodes = pd.DataFrame([])\n",
    "covid_recovery_nodes = pd.DataFrame([])\n",
    "covid_recovery_deaths = pd.DataFrame([])\n",
    "covid_recovery_new_cases = pd.DataFrame([])\n",
    "if covid_ts_path.exists():\n",
    "    covid_ts = pd.read_csv(covid_ts_path)\n",
    "    if 'location' not in covid_ts.columns and 'country' in covid_ts.columns:\n",
    "        covid_ts['location'] = covid_ts['country']\n",
    "    covid_ts = covid_ts[['location','date','total_cases','total_deaths','new_cases','population']]\n",
    "    covid_ts = covid_ts.dropna(subset=['total_cases','total_deaths','population'])\n",
    "    covid_ts['year'] = pd.to_datetime(covid_ts['date']).dt.year\n",
    "    covid_ts['id'] = covid_ts['location'].str.lower().str.replace(' ', '_')\n",
    "    covid_ts = covid_ts[covid_ts['id'].isin(covid['id'])]\n",
    "    covid_annual = covid_ts.groupby(['id','location','year']).agg({\n",
    "        'total_cases': 'max',\n",
    "        'total_deaths': 'max',\n",
    "        'new_cases': 'sum',\n",
    "        'population': 'max',\n",
    "    }).reset_index()\n",
    "    covid_annual['cases_per_m'] = covid_annual['total_cases'] / (covid_annual['population'] / 1_000_000)\n",
    "    covid_annual['deaths_per_m'] = covid_annual['total_deaths'] / (covid_annual['population'] / 1_000_000)\n",
    "    covid_annual['new_cases_per_m'] = covid_annual['new_cases'] / (covid_annual['population'] / 1_000_000)\n",
    "    covid_annual = covid_annual[covid_annual['year'] >= 2019]\n",
    "    covid_year_nodes = pd.DataFrame({\n",
    "        'id': covid_annual.apply(lambda r: f'covid_year:{r.id}:{int(r.year)}', axis=1),\n",
    "        'label': covid_annual.apply(lambda r: f'COVID {int(r.year)} ({r.location})', axis=1),\n",
    "        'type': 'covid_year',\n",
    "        'attributes': covid_annual.apply(lambda r: {\n",
    "            'year': int(r.year),\n",
    "            'total_cases': float(r.total_cases),\n",
    "            'total_deaths': float(r.total_deaths),\n",
    "            'cases_per_m': float(r.cases_per_m),\n",
    "            'deaths_per_m': float(r.deaths_per_m),\n",
    "            'population': float(r.population),\n",
    "            'country_id': r.id,\n",
    "            'country_name': r.location,\n",
    "        }, axis=1),\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    covid_peak_nodes = []\n",
    "    for cid, grp in covid_annual.groupby('id'):\n",
    "        g = grp.sort_values('year')\n",
    "        peak_idx = g['cases_per_m'].idxmax()\n",
    "        peak = g.loc[peak_idx]\n",
    "        covid_peak_nodes.append({\n",
    "            'id': f'covid_peak:{cid}',\n",
    "            'label': f\"COVID peak {int(peak['year'])} ({peak['location']})\",\n",
    "            'type': 'covid_peak',\n",
    "            'attributes': {\n",
    "                'year': int(peak['year']),\n",
    "                'peak_cases_per_m': float(peak['cases_per_m']),\n",
    "                'country_id': cid,\n",
    "                'country_name': peak['location'],\n",
    "            },\n",
    "        })\n",
    "    covid_peak_nodes = pd.DataFrame(covid_peak_nodes)\n",
    "\n",
    "\n",
    "    covid_recovery_nodes = []\n",
    "    covid_recovery_deaths = []\n",
    "    covid_recovery_new_cases = []\n",
    "\n",
    "    for cid, grp in covid_annual.groupby('id'):\n",
    "        g = grp.sort_values('year')\n",
    "\n",
    "        # Peak and recovery based on cases_per_m (legacy)\n",
    "        peak_idx = g['cases_per_m'].idxmax()\n",
    "        peak = g.loc[peak_idx]\n",
    "        peak_year = int(peak['year'])\n",
    "        peak_cases = float(peak['cases_per_m'])\n",
    "        threshold_cases = peak_cases * 0.9\n",
    "        after = g[g['year'] > peak_year]\n",
    "        rec = after[after['cases_per_m'] <= threshold_cases]\n",
    "        if not rec.empty:\n",
    "            rec_row = rec.iloc[0]\n",
    "            covid_recovery_nodes.append({\n",
    "                'id': f'covid_recovery:{cid}',\n",
    "                'label': f\"COVID recovery {int(rec_row['year'])} ({rec_row['location']})\",\n",
    "                'type': 'covid_recovery',\n",
    "                'attributes': {\n",
    "                    'year': int(rec_row['year']),\n",
    "                    'peak_year': peak_year,\n",
    "                    'peak_cases_per_m': peak_cases,\n",
    "                    'recovery_cases_per_m': float(rec_row['cases_per_m']),\n",
    "                    'country_id': cid,\n",
    "                    'country_name': rec_row['location'],\n",
    "                },\n",
    "            })\n",
    "\n",
    "        # Recovery based on deaths_per_m\n",
    "        peak_d_idx = g['deaths_per_m'].idxmax()\n",
    "        peak_d = g.loc[peak_d_idx]\n",
    "        peak_d_year = int(peak_d['year'])\n",
    "        peak_deaths = float(peak_d['deaths_per_m'])\n",
    "        threshold_deaths = peak_deaths * 0.7\n",
    "        after_d = g[g['year'] > peak_d_year]\n",
    "        rec_d = after_d[after_d['deaths_per_m'] <= threshold_deaths]\n",
    "        if not rec_d.empty:\n",
    "            rec_row = rec_d.iloc[0]\n",
    "            covid_recovery_deaths.append({\n",
    "                'id': f'covid_recovery_deaths:{cid}',\n",
    "                'label': f\"COVID deaths recovery {int(rec_row['year'])} ({rec_row['location']})\",\n",
    "                'type': 'covid_recovery_deaths',\n",
    "                'attributes': {\n",
    "                    'year': int(rec_row['year']),\n",
    "                    'peak_year': peak_d_year,\n",
    "                    'peak_deaths_per_m': peak_deaths,\n",
    "                    'recovery_deaths_per_m': float(rec_row['deaths_per_m']),\n",
    "                    'country_id': cid,\n",
    "                    'country_name': rec_row['location'],\n",
    "                },\n",
    "            })\n",
    "\n",
    "        # Recovery based on new_cases_per_m (non-cumulative)\n",
    "        peak_n_idx = g['new_cases_per_m'].idxmax()\n",
    "        peak_n = g.loc[peak_n_idx]\n",
    "        peak_n_year = int(peak_n['year'])\n",
    "        peak_new_cases = float(peak_n['new_cases_per_m'])\n",
    "        threshold_new = peak_new_cases * 0.5\n",
    "        after_n = g[g['year'] > peak_n_year]\n",
    "        rec_n = after_n[after_n['new_cases_per_m'] <= threshold_new]\n",
    "        if not rec_n.empty:\n",
    "            rec_row = rec_n.iloc[0]\n",
    "            covid_recovery_new_cases.append({\n",
    "                'id': f'covid_recovery_new_cases:{cid}',\n",
    "                'label': f\"COVID new-cases recovery {int(rec_row['year'])} ({rec_row['location']})\",\n",
    "                'type': 'covid_recovery_new_cases',\n",
    "                'attributes': {\n",
    "                    'year': int(rec_row['year']),\n",
    "                    'peak_year': peak_n_year,\n",
    "                    'peak_new_cases_per_m': peak_new_cases,\n",
    "                    'recovery_new_cases_per_m': float(rec_row['new_cases_per_m']),\n",
    "                    'country_id': cid,\n",
    "                    'country_name': rec_row['location'],\n",
    "                },\n",
    "            })\n",
    "\n",
    "    covid_recovery_nodes = pd.DataFrame(covid_recovery_nodes)\n",
    "    covid_recovery_deaths = pd.DataFrame(covid_recovery_deaths)\n",
    "    covid_recovery_new_cases = pd.DataFrame(covid_recovery_new_cases)\n",
    "\n",
    "gdp_year_nodes = build_year_nodes(wb_gdp, 'NY.GDP.MKTP.CD', 'gdp', max_years=12)\n",
    "infl_year_nodes = build_year_nodes(wb_infl, 'FP.CPI.TOTL.ZG', 'inflation', max_years=12)\n",
    "unemp_year_nodes = build_year_nodes(wb_unemp, 'SL.UEM.TOTL.ZS', 'unemployment', max_years=12)\n",
    "year_nodes = pd.concat([gdp_year_nodes, infl_year_nodes, unemp_year_nodes], ignore_index=True)\n",
    "recovery_nodes = pd.concat([\n",
    "    build_recovery_nodes(wb_gdp, 'NY.GDP.MKTP.CD', 'gdp', higher_is_better=True),\n",
    "    build_recovery_nodes(wb_infl, 'FP.CPI.TOTL.ZG', 'inflation', higher_is_better=False),\n",
    "    build_recovery_nodes(wb_unemp, 'SL.UEM.TOTL.ZS', 'unemployment', higher_is_better=False),\n",
    "], ignore_index=True)\n",
    "\n",
    "nodes = pd.concat([\n",
    "    covid_nodes,\n",
    "    fin_nodes,\n",
    "    wb_nodes.drop(columns=['country_code','country_name']),\n",
    "    wiki_nodes.drop(columns=['keywords']),\n",
    "    burden_nodes,\n",
    "    bucket_nodes,\n",
    "    year_nodes,\n",
    "    recovery_nodes,\n",
    "    covid_year_nodes,\n",
    "    covid_peak_nodes,\n",
    "    covid_recovery_nodes,\n",
    "    covid_recovery_deaths,\n",
    "    covid_recovery_new_cases,\n",
    "], ignore_index=True)\n",
    "nodes.to_parquet(processed / 'nodes.parquet', index=False)\n",
    "print('Wrote nodes.parquet:', len(nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2497f6b",
   "metadata": {},
   "source": [
    "## Build Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9aeed48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:42.006634Z",
     "iopub.status.busy": "2026-02-07T07:44:42.006634Z",
     "iopub.status.idle": "2026-02-07T07:44:42.615410Z",
     "shell.execute_reply": "2026-02-07T07:44:42.615410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote edges.parquet: 17287\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "for _, row in covid.iterrows():\n",
    "    edges.append({\n",
    "        'source': row.id,\n",
    "        'target': row.id,\n",
    "        'relation': 'cases_in',\n",
    "        'weight': min(float(row.cases_per_m / 1_000_000), 1.0),\n",
    "        'confidence': 0.8,\n",
    "        'causal_type': 'correlational',\n",
    "        'provenance': 'observed',\n",
    "    })\n",
    "    edges.append({\n",
    "        'source': row.id,\n",
    "        'target': f'covid_burden_{row.id}',\n",
    "        'relation': 'has_burden',\n",
    "        'weight': min(float(row.deaths_per_m / 10000), 1.0),\n",
    "        'confidence': 0.8,\n",
    "        'causal_type': 'correlational',\n",
    "        'provenance': 'observed',\n",
    "    })\n",
    "    pop = float(row.population)\n",
    "    if pop < 5_000_000:\n",
    "        bucket = 'pop_lt_5m'\n",
    "    elif pop < 20_000_000:\n",
    "        bucket = 'pop_5m_20m'\n",
    "    elif pop < 100_000_000:\n",
    "        bucket = 'pop_20m_100m'\n",
    "    else:\n",
    "        bucket = 'pop_gt_100m'\n",
    "    edges.append({\n",
    "        'source': row.id,\n",
    "        'target': bucket,\n",
    "        'relation': 'has_population_bucket',\n",
    "        'weight': 0.7,\n",
    "        'confidence': 0.7,\n",
    "        'causal_type': 'correlational',\n",
    "        'provenance': 'observed',\n",
    "    })\n",
    "\n",
    "# link tickers to countries for demo\n",
    "for sym in fin_nodes['id'].tolist():\n",
    "    for cid in covid['id'].head(30):\n",
    "        edges.append({\n",
    "            'source': sym,\n",
    "            'target': cid,\n",
    "            'relation': 'market_exposure',\n",
    "            'weight': 0.5,\n",
    "            'confidence': 0.6,\n",
    "            'causal_type': 'hypothetical',\n",
    "            'provenance': 'inferred',\n",
    "        })\n",
    "        edges.append({\n",
    "            'source': sym,\n",
    "            'target': f'covid_burden_{cid}',\n",
    "            'relation': 'exposed_to_burden',\n",
    "            'weight': 0.4,\n",
    "            'confidence': 0.5,\n",
    "            'causal_type': 'hypothetical',\n",
    "            'provenance': 'inferred',\n",
    "        })\n",
    "\n",
    "# link WB indicators to countries\n",
    "for _, r in wb_nodes.iterrows():\n",
    "    cname = r['country_name']\n",
    "    cid = cname.lower().replace(' ', '_')\n",
    "    if cid in covid['id'].values:\n",
    "        edges.append({\n",
    "            'source': r['id'],\n",
    "            'target': cid,\n",
    "            'relation': 'indicator_of',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'correlational',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "\n",
    "# link year nodes to countries and indicators\n",
    "for _, n in year_nodes.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    indicator_id = attrs.get('indicator_id')\n",
    "    if cid:\n",
    "        edges.append({\n",
    "            'source': cid,\n",
    "            'target': n['id'],\n",
    "            'relation': f\"has_{n['type']}\",\n",
    "            'weight': 0.7,\n",
    "            'confidence': 0.7,\n",
    "            'causal_type': 'observed',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "    if indicator_id:\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': indicator_id,\n",
    "            'relation': 'year_of_indicator',\n",
    "            'weight': 0.7,\n",
    "            'confidence': 0.7,\n",
    "            'causal_type': 'observed',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "# link GDP recovery nodes to countries and indicators\n",
    "for _, n in recovery_nodes.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    indicator_id = attrs.get('indicator_id')\n",
    "    if cid:\n",
    "        edges.append({\n",
    "            'source': cid,\n",
    "            'target': n['id'],\n",
    "            'relation': 'gdp_recovered_in',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "    if indicator_id:\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': indicator_id,\n",
    "            'relation': 'recovery_for_indicator',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "\n",
    "# link COVID recovery nodes to countries and year nodes\n",
    "for _, n in covid_recovery_nodes.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    year = attrs.get('year')\n",
    "    if cid:\n",
    "        edges.append({\n",
    "            'source': cid,\n",
    "            'target': n['id'],\n",
    "            'relation': 'covid_recovered_in',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "    if cid and year:\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': f'covid_year:{cid}:{year}',\n",
    "            'relation': 'recovery_year',\n",
    "            'weight': 0.7,\n",
    "            'confidence': 0.7,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "# link COVID year nodes to economic year nodes (same country/year)\n",
    "year_node_ids = set(year_nodes['id']) if 'year_nodes' in globals() else set()\n",
    "for _, n in covid_year_nodes.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    year = attrs.get('year')\n",
    "    if not cid or year is None:\n",
    "        continue\n",
    "    for prefix in ['gdp', 'inflation', 'unemployment']:\n",
    "        target = f'{prefix}_year:{cid}:{year}'\n",
    "        if target in year_node_ids:\n",
    "            edges.append({\n",
    "                'source': n['id'],\n",
    "                'target': target,\n",
    "                'relation': f'covid_vs_{prefix}_year',\n",
    "                'weight': 0.6,\n",
    "                'confidence': 0.6,\n",
    "                'causal_type': 'inferred',\n",
    "                'provenance': 'observed',\n",
    "            })\n",
    "\n",
    "\n",
    "# link COVID peak nodes to countries and year nodes\n",
    "for _, n in covid_peak_nodes.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    year = attrs.get('year')\n",
    "    if cid:\n",
    "        edges.append({\n",
    "            'source': cid,\n",
    "            'target': n['id'],\n",
    "            'relation': 'covid_peak_in',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'observed',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "    if cid and year:\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': f'covid_year:{cid}:{year}',\n",
    "            'relation': 'peak_year',\n",
    "            'weight': 0.7,\n",
    "            'confidence': 0.7,\n",
    "            'causal_type': 'observed',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "\n",
    "# link COVID recovery to economic recovery nodes (same country)\n",
    "for _, n in covid_recovery_nodes.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    if not cid:\n",
    "        continue\n",
    "    for prefix in ['gdp', 'inflation', 'unemployment']:\n",
    "        target = f'{prefix}_recovery:{cid}'\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': target,\n",
    "            'relation': f'covid_vs_{prefix}_recovery',\n",
    "            'weight': 0.6,\n",
    "            'confidence': 0.6,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "\n",
    "# link COVID deaths/new-cases recovery nodes to countries\n",
    "for _, n in covid_recovery_deaths.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    year = attrs.get('year')\n",
    "    if cid:\n",
    "        edges.append({\n",
    "            'source': cid,\n",
    "            'target': n['id'],\n",
    "            'relation': 'covid_deaths_recovered_in',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "    if cid and year:\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': f'covid_year:{cid}:{year}',\n",
    "            'relation': 'recovery_year',\n",
    "            'weight': 0.7,\n",
    "            'confidence': 0.7,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "for _, n in covid_recovery_new_cases.iterrows():\n",
    "    attrs = n.get('attributes', {})\n",
    "    cid = attrs.get('country_id')\n",
    "    year = attrs.get('year')\n",
    "    if cid:\n",
    "        edges.append({\n",
    "            'source': cid,\n",
    "            'target': n['id'],\n",
    "            'relation': 'covid_new_cases_recovered_in',\n",
    "            'weight': 0.8,\n",
    "            'confidence': 0.8,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "    if cid and year:\n",
    "        edges.append({\n",
    "            'source': n['id'],\n",
    "            'target': f'covid_year:{cid}:{year}',\n",
    "            'relation': 'recovery_year',\n",
    "            'weight': 0.7,\n",
    "            'confidence': 0.7,\n",
    "            'causal_type': 'inferred',\n",
    "            'provenance': 'observed',\n",
    "        })\n",
    "\n",
    "# link wiki topics using keyword extraction\n",
    "country_labels = {r['label']: r['id'] for _, r in covid_nodes.iterrows()}\n",
    "for _, row in wiki_nodes.iterrows():\n",
    "    title = row['label']\n",
    "    t = title.lower()\n",
    "    kws = keywords(title)\n",
    "    for cname, cid in country_labels.items():\n",
    "        if cname.lower() in t:\n",
    "            edges.append({\n",
    "                'source': row['id'],\n",
    "                'target': cid,\n",
    "                'relation': 'mentions_country',\n",
    "                'weight': 0.4,\n",
    "                'confidence': 0.5,\n",
    "                'causal_type': 'hypothetical',\n",
    "                'provenance': 'inferred',\n",
    "            })\n",
    "    for kw in kws:\n",
    "        if kw in ['covid', 'pandemic', 'health', 'disease']:\n",
    "            for cid in covid['id'].head(40):\n",
    "                edges.append({\n",
    "                    'source': row['id'],\n",
    "                    'target': f'covid_burden_{cid}',\n",
    "                    'relation': 'mentions_pandemic',\n",
    "                    'weight': 0.3,\n",
    "                    'confidence': 0.4,\n",
    "                    'causal_type': 'hypothetical',\n",
    "                    'provenance': 'inferred',\n",
    "                })\n",
    "        if kw in ['gdp', 'economy', 'finance', 'market', 'inflation', 'unemployment']:\n",
    "            for ind in wb_nodes['id'].head(20):\n",
    "                edges.append({\n",
    "                    'source': row['id'],\n",
    "                    'target': ind,\n",
    "                    'relation': 'mentions_economy',\n",
    "                    'weight': 0.3,\n",
    "                    'confidence': 0.4,\n",
    "                    'causal_type': 'hypothetical',\n",
    "                    'provenance': 'inferred',\n",
    "                })\n",
    "\n",
    "# link countries by similarity in burden (top 4 neighbors)\n",
    "covid_sorted = covid.sort_values('cases_per_m')\n",
    "ids = covid_sorted['id'].tolist()\n",
    "for i in range(len(ids)):\n",
    "    for j in range(1, 5):\n",
    "        if i + j < len(ids):\n",
    "            edges.append({\n",
    "                'source': ids[i],\n",
    "                'target': ids[i+j],\n",
    "                'relation': 'similar_burden',\n",
    "                'weight': 0.4,\n",
    "                'confidence': 0.5,\n",
    "                'causal_type': 'correlational',\n",
    "                'provenance': 'inferred',\n",
    "            })\n",
    "\n",
    "edges = pd.DataFrame(edges)\n",
    "edges.to_parquet(processed / 'edges.parquet', index=False)\n",
    "print('Wrote edges.parquet:', len(edges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326a08f",
   "metadata": {},
   "source": [
    "## Embeddings (random demo vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9071b6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:42.615410Z",
     "iopub.status.busy": "2026-02-07T07:44:42.615410Z",
     "iopub.status.idle": "2026-02-07T07:44:42.917451Z",
     "shell.execute_reply": "2026-02-07T07:44:42.917451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote embeddings.parquet: 27094\n",
      " - nodes: 9807\n",
      " - edges: 17287\n"
     ]
    }
   ],
   "source": [
    "dim = 16\n",
    "node_emb = pd.DataFrame({\n",
    "    'id': nodes['id'],\n",
    "    'kind': 'node',\n",
    "    'vector': [list(np.random.rand(dim)) for _ in range(len(nodes))]\n",
    "})\n",
    "edge_ids = edges.apply(\n",
    "    lambda r: f\"{r['source']}|{r['relation']}|{r['target']}\",\n",
    "    axis=1,\n",
    ")\n",
    "edge_emb = pd.DataFrame({\n",
    "    'id': edge_ids,\n",
    "    'kind': 'edge',\n",
    "    'vector': [list(np.random.rand(dim)) for _ in range(len(edges))]\n",
    "})\n",
    "emb = pd.concat([node_emb, edge_emb], ignore_index=True)\n",
    "emb.to_parquet(processed / 'embeddings.parquet', index=False)\n",
    "print('Wrote embeddings.parquet:', len(emb))\n",
    "print(' - nodes:', len(node_emb))\n",
    "print(' - edges:', len(edge_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03602622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:42.917451Z",
     "iopub.status.busy": "2026-02-07T07:44:42.917451Z",
     "iopub.status.idle": "2026-02-07T07:44:43.020954Z",
     "shell.execute_reply": "2026-02-07T07:44:43.020954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "unemployment_year           2811\n",
       "inflation_year              2765\n",
       "covid_year                  1708\n",
       "wb_indicator                 562\n",
       "wiki_topic                   500\n",
       "country                      246\n",
       "covid_burden                 246\n",
       "covid_peak                   244\n",
       "covid_recovery_new_cases     244\n",
       "unemployment_recovery        174\n",
       "inflation_recovery           159\n",
       "gdp_year                     120\n",
       "gdp_recovery                   9\n",
       "covid_recovery_deaths          7\n",
       "ticker                         6\n",
       "population_bucket              4\n",
       "covid_recovery                 2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes=pd.read_parquet(processed / 'nodes.parquet')\n",
    "nodes[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afb1c3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T07:44:43.024638Z",
     "iopub.status.busy": "2026-02-07T07:44:43.024638Z",
     "iopub.status.idle": "2026-02-07T07:44:43.052172Z",
     "shell.execute_reply": "2026-02-07T07:44:43.052172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>aapl</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>ticker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id label    type\n",
       "246  aapl  AAPL  ticker"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.query(\"id.str.contains('aapl')\")[[\"id\", \"label\", \"type\"]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
